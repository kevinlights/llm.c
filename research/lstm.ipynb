{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (10) to match target batch_size (33).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 55\u001b[0m\n\u001b[1;32m     53\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     54\u001b[0m output \u001b[38;5;241m=\u001b[39m model(tensor_x)\n\u001b[0;32m---> 55\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     57\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/projects/llm.c/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/llm.c/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/llm.c/.venv/lib/python3.10/site-packages/torch/nn/modules/loss.py:1185\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/llm.c/.venv/lib/python3.10/site-packages/torch/nn/functional.py:3086\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3085\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3086\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (10) to match target batch_size (33)."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 假设我们有一个非常小的数据集\n",
    "corpus = 'hello world this is a simple example corpus'\n",
    "chars = sorted(list(set(corpus)))\n",
    "vocab_size = len(chars)\n",
    "char_to_index = dict((c, i) for i, c in enumerate(chars))\n",
    "index_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# 准备数据\n",
    "seq_length = 10\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(corpus) - seq_length):\n",
    "    seq_in = corpus[i:i + seq_length]\n",
    "    seq_out = corpus[i + seq_length]\n",
    "    dataX.append([char_to_index[char] for char in seq_in])\n",
    "    dataY.append(char_to_index[seq_out])\n",
    "\n",
    "# 转换数据为PyTorch张量\n",
    "tensor_x = torch.tensor(dataX, dtype=torch.long).cuda()\n",
    "tensor_y = torch.tensor(dataY, dtype=torch.long).cuda()\n",
    "\n",
    "# 创建LSTM模型\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        return self.fc(hidden[-1])\n",
    "\n",
    "# 设置模型参数\n",
    "embedding_dim = 32\n",
    "hidden_dim = 64\n",
    "output_dim = vocab_size\n",
    "\n",
    "model = LSTMModel(vocab_size, embedding_dim, hidden_dim, output_dim).cuda()\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.zero_grad()\n",
    "    output = model(tensor_x)\n",
    "    loss = criterion(output, tensor_y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# 生成文本\n",
    "def generate_text(model, start_char, gen_size=50):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input = torch.tensor([[char_to_index[start_char]]], dtype=torch.long).cuda()\n",
    "        output_chars = [start_char]\n",
    "\n",
    "        for _ in range(gen_size):\n",
    "            output = model(input)\n",
    "            _, top_i = output.topk(1)\n",
    "            char = index_to_char[top_i.item()]\n",
    "            output_chars.append(char)\n",
    "\n",
    "            input = torch.tensor([[top_i.item()]], dtype=torch.long).cuda()\n",
    "\n",
    "        return ''.join(output_chars)\n",
    "\n",
    "# 使用模型生成文本\n",
    "start_char = 'h'\n",
    "generated_text = generate_text(model, start_char)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.6315\n",
      "Epoch [20/100], Loss: 0.1015\n",
      "Epoch [30/100], Loss: 0.0368\n",
      "Epoch [40/100], Loss: 0.0186\n",
      "Epoch [50/100], Loss: 0.0112\n",
      "Epoch [60/100], Loss: 0.0074\n",
      "Epoch [70/100], Loss: 0.0053\n",
      "Epoch [80/100], Loss: 0.0038\n",
      "Epoch [90/100], Loss: 0.0029\n",
      "Epoch [100/100], Loss: 0.0022\n",
      "his s s s s s s s s s s s s s s s s s s s s s s s s\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 假设我们有一个非常小的数据集\n",
    "corpus = 'hello world this is a simple example corpus'\n",
    "chars = sorted(list(set(corpus)))\n",
    "vocab_size = len(chars)\n",
    "char_to_index = dict((c, i) for i, c in enumerate(chars))\n",
    "index_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# 准备数据\n",
    "seq_length = 10\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(corpus) - seq_length):\n",
    "    seq_in = corpus[i:i + seq_length]\n",
    "    seq_out = corpus[i + seq_length]\n",
    "    dataX.append([char_to_index[char] for char in seq_in])\n",
    "    dataY.append(char_to_index[seq_out])\n",
    "\n",
    "# 转换数据为PyTorch张量\n",
    "tensor_x = torch.tensor(dataX, dtype=torch.long).cuda()\n",
    "tensor_y = torch.tensor(dataY, dtype=torch.long).cuda()\n",
    "\n",
    "# 创建LSTM模型\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        return self.fc(hidden[-1])\n",
    "\n",
    "# 设置模型参数\n",
    "embedding_dim = 32\n",
    "hidden_dim = 64\n",
    "output_dim = vocab_size\n",
    "\n",
    "model = LSTMModel(vocab_size, embedding_dim, hidden_dim, output_dim).cuda()\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, tensor_x.size(0), 1):  # Adjusted to loop through one sequence at a time\n",
    "        model.zero_grad()\n",
    "        output = model(tensor_x[i].view(-1, 1))  # Adjusted to handle one sequence at a time\n",
    "        loss = criterion(output, tensor_y[i].view(1))  # Adjusted to handle one target at a time\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# 生成文本\n",
    "def generate_text(model, start_char, gen_size=50):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input = torch.tensor([[char_to_index[start_char]]], dtype=torch.long).cuda()\n",
    "        output_chars = [start_char]\n",
    "\n",
    "        for _ in range(gen_size):\n",
    "            output = model(input)\n",
    "            _, top_i = output.topk(1)\n",
    "            char = index_to_char[top_i.item()]\n",
    "            output_chars.append(char)\n",
    "\n",
    "            input = torch.tensor([[top_i.item()]], dtype=torch.long).cuda()\n",
    "\n",
    "        return ''.join(output_chars)\n",
    "\n",
    "# 使用模型生成文本\n",
    "start_char = 'h'\n",
    "generated_text = generate_text(model, start_char)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 1.1415\n",
      "Epoch [20/100], Loss: 0.2357\n",
      "Epoch [30/100], Loss: 0.0667\n",
      "Epoch [40/100], Loss: 0.0308\n",
      "Epoch [50/100], Loss: 0.0176\n",
      "Epoch [60/100], Loss: 0.0113\n",
      "Epoch [70/100], Loss: 0.0077\n",
      "Epoch [80/100], Loss: 0.0055\n",
      "Epoch [90/100], Loss: 0.0041\n",
      "Epoch [100/100], Loss: 0.0031\n",
      "hrpus corpus corpus corpus corpus corpus corpus cor\n"
     ]
    }
   ],
   "source": [
    "# 首先，我们需要一些基本的库来构建和训练我们的模型。torch是PyTorch的主库，它提供了构建神经网络所需的所有工具。torch.nn是PyTorch的神经网络库，它包含了许多预先定义好的层和函数，我们可以直接使用它们来构建模型。torch.optim是PyTorch的优化库，它包含了许多优化算法，我们可以使用它们来更新模型的权重。\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 接下来，我们定义了一个非常小的文本数据集，这个数据集就是我们的“语料库”。我们从这个语料库中创建了一个字符到索引的映射，这样我们就可以将文本字符转换为数字，因为神经网络只能处理数字输入。\n",
    "# 假设我们有一个非常小的数据集\n",
    "corpus = 'hello world this is a simple example corpus'\n",
    "chars = sorted(list(set(corpus)))\n",
    "vocab_size = len(chars)\n",
    "char_to_index = dict((c, i) for i, c in enumerate(chars))\n",
    "index_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# 然后，我们准备我们的数据。我们决定每个序列的长度是10个字符，这意味着我们的模型将会每次看到10个字符，并尝试预测这10个字符之后的下一个字符。我们为每个这样的序列创建了一个输入和一个目标输出。\n",
    "# 准备数据\n",
    "seq_length = 10\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(corpus) - seq_length):\n",
    "    seq_in = corpus[i:i + seq_length]\n",
    "    seq_out = corpus[i + seq_length]\n",
    "    dataX.append([char_to_index[char] for char in seq_in])\n",
    "    dataY.append(char_to_index[seq_out])\n",
    "\n",
    "# 我们将这些输入和输出转换为PyTorch张量，并将它们移动到GPU上（如果有的话）以加速训练过程。\n",
    "# 转换数据为PyTorch张量\n",
    "tensor_x = torch.tensor(dataX, dtype=torch.long).cuda()\n",
    "tensor_y = torch.tensor(dataY, dtype=torch.long).cuda()\n",
    "\n",
    "# 接下来，我们定义了我们的LSTM模型。这个模型包含一个嵌入层，它将字符索引转换为向量表示；一个LSTM层，它将处理这些向量序列；和一个全连接层，它将LSTM层的输出转换为每个可能字符的分数\n",
    "# 创建LSTM模型\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        return self.fc(hidden[-1])\n",
    "\n",
    "# 我们设置了一些模型参数，并创建了一个模型实例。我们还定义了损失函数和优化器，这些都是训练神经网络所必需的。\n",
    "# 设置模型参数\n",
    "embedding_dim = 32\n",
    "hidden_dim = 64\n",
    "output_dim = vocab_size\n",
    "\n",
    "model = LSTMModel(vocab_size, embedding_dim, hidden_dim, output_dim).cuda()\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# 然后，我们开始训练模型。我们决定训练100个周期，每个周期我们都将遍历整个数据集一次。在每次迭代中，我们将一个序列输入模型，并计算模型预测的下一个字符与实际下一个字符之间的差异。然后，我们使用这个差异来更新模型的权重。\n",
    "# 训练模型\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, tensor_x.size(0), 1):  # Adjusted to loop through one sequence at a time\n",
    "        model.zero_grad()\n",
    "        output = model(tensor_x[i].view(-1, 1))  # Adjusted to handle one sequence at a time\n",
    "        loss = criterion(output, tensor_y[i].view(1))  # Adjusted to handle one target at a time\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# 最后，我们定义了一个函数来生成文本。这个函数接受一个起始字符，并使用模型来预测接下来的50个字符。它将每个预测的字符添加到文本中，并使用这个新的文本片段作为输入来预测下一个字符。\n",
    "# 生成文本\n",
    "def generate_text(model, start_char, gen_size=50):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input = torch.tensor([[char_to_index[start_char]]], dtype=torch.long).cuda()\n",
    "        output_chars = [start_char]\n",
    "\n",
    "        for _ in range(gen_size):\n",
    "            output = model(input)\n",
    "            _, top_i = output.topk(1)\n",
    "            char = index_to_char[top_i.item()]\n",
    "            output_chars.append(char)\n",
    "\n",
    "            input = torch.tensor([[top_i.item()]], dtype=torch.long).cuda()\n",
    "\n",
    "        return ''.join(output_chars)\n",
    "\n",
    "# 使用模型生成文本\n",
    "start_char = 'h'\n",
    "generated_text = generate_text(model, start_char)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
